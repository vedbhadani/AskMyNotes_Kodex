# ğŸ“š AskMyNotes

> **AI-powered note assistant** â€” Upload your PDF or TXT notes, and ask questions, generate study dashboards, or transcribe voice queries â€” all grounded strictly in your own uploaded material.

---

## ğŸš€ Features

| Feature | Description |
|---|---|
| ğŸ“‚ **Subject Management** | Create up to 3 subjects, each with a name, icon, and colour theme |
| ğŸ“„ **File Upload** | Upload up to 20 PDF / TXT files per subject (max 50 MB each) |
| ğŸ¤– **AI Chat (Groq)** | Ask questions answered *only* from your uploaded notes via Llama 3.3 70B |
| ğŸ“Š **Study Dashboard** | Auto-generates a Markdown summary, 5 MCQs, and 3 flashcard questions from your notes |
| ğŸ™ï¸ **Voice Transcription** | Record audio questions â€” transcribed locally using OpenAI Whisper (Tiny EN, ONNX) |
| â˜ï¸ **Cloudinary Storage** | Raw files are backed up to Cloudinary after text extraction |
| ğŸ—„ï¸ **MongoDB Persistence** | Subjects, files, and chat history are stored in MongoDB Atlas |
| ğŸ” **Auth (localStorage)** | Client-side sign-up / sign-in with session persistence |
| ğŸ“¤ **PDF Export** | Export study notes as a PDF via jsPDF |
| ğŸŒ“ **Cyber-Minimalist UI** | Dark-mode, neon-accent, glassmorphism design built in Vanilla CSS + React |

---

## ğŸ—ï¸ Project Structure

```
AskMyNotes/
â”œâ”€â”€ backend/                  # Node.js / Express API
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ db.js             # MongoDB connection (Mongoose)
â”‚   â”‚   â””â”€â”€ cloudinary.js     # Cloudinary SDK configuration
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ Subject.js        # Subject schema (subjectId, name, icon, color)
â”‚   â”‚   â”œâ”€â”€ File.js           # File schema (fileName, extractedText, cloudinaryUrl)
â”‚   â”‚   â””â”€â”€ ChatHistory.js    # Chat log schema (subjectId, question, response)
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ upload.js         # POST /api/upload, POST /api/clear-subject
â”‚   â”‚   â”œâ”€â”€ chat.js           # POST /api/chat
â”‚   â”‚   â”œâ”€â”€ study.js          # POST /api/study-mode
â”‚   â”‚   â””â”€â”€ transcribe.js     # POST /api/transcribe
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ getSubjectText.js # Aggregates extracted text for a subject from MongoDB
â”‚   â”œâ”€â”€ uploads/              # Temp storage for incoming files (auto-cleaned)
â”‚   â”œâ”€â”€ .cache/               # Whisper ONNX model cache (~150 MB, auto-downloaded)
â”‚   â”œâ”€â”€ .env                  # Environment variables (see below)
â”‚   â”œâ”€â”€ server.js             # App entry point
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ frontend/                 # React + Vite SPA
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ context/
    â”‚   â”‚   â”œâ”€â”€ AppContext.jsx # Global subject/page state (up to 3 subjects)
    â”‚   â”‚   â””â”€â”€ AuthContext.jsx # localStorage-based auth (sign-up / sign-in / sign-out)
    â”‚   â”œâ”€â”€ pages/
    â”‚   â”‚   â”œâ”€â”€ SetupPage.jsx  # Subject creation & file upload UI
    â”‚   â”‚   â”œâ”€â”€ StudyPage.jsx  # Study dashboard (summary, MCQs, flashcards)
    â”‚   â”‚   â”œâ”€â”€ ChatPage.jsx   # AI chat with voice input + markdown rendering
    â”‚   â”‚   â”œâ”€â”€ SignInPage.jsx # Login form
    â”‚   â”‚   â””â”€â”€ SignUpPage.jsx # Registration form
    â”‚   â”œâ”€â”€ components/
    â”‚   â”‚   â””â”€â”€ Sidebar.jsx   # Navigation sidebar with subject list
    â”‚   â”œâ”€â”€ utils/            # Helper utilities (e.g. PDF export)
    â”‚   â”œâ”€â”€ App.jsx           # Root component with AuthGate + routing
    â”‚   â””â”€â”€ main.jsx          # React DOM entry point
    â”œâ”€â”€ index.html
    â”œâ”€â”€ vite.config.js
    â””â”€â”€ package.json
```

---

## âš™ï¸ Tech Stack

### Backend
| Package | Version | Purpose |
|---|---|---|
| `express` | ^5.2 | HTTP server & routing |
| `mongoose` | ^8.13 | MongoDB ODM |
| `multer` | ^2.0 | Multipart file uploads |
| `pdf-parse` | ^2.4 | PDF text extraction |
| `groq-sdk` | ^0.37 | Groq AI (Llama 3.3 70B) |
| `@huggingface/transformers` | ^3.8 | Whisper ASR (local ONNX) |
| `cloudinary` | ^2.5 | File CDN storage |
| `cors` | ^2.8 | Cross-origin requests |
| `dotenv` | ^17.3 | Environment variable loader |

### Frontend
| Package | Version | Purpose |
|---|---|---|
| `react` + `react-dom` | ^19.2 | UI framework |
| `react-router-dom` | ^7.13 | Client-side routing |
| `vite` + `@vitejs/plugin-react` | ^7.3 / ^5.1 | Build tool & HMR |
| `lucide-react` | ^0.575 | Icon set |
| `marked` | ^17.0 | Markdown â†’ HTML rendering |
| `jspdf` | ^4.2 | PDF export |

### External Services
| Service | Used For |
|---|---|
| **MongoDB Atlas** | Subjects, files, and chat history |
| **Cloudinary** | Raw file CDN backup |
| **Groq API** | AI completions (Llama 3.3 70B Versatile) |
| **Hugging Face (ONNX)** | `whisper-tiny.en` â€” local voice transcription |

> **Note:** `ffmpeg` must be installed on the system PATH for voice transcription to work.

---

## ğŸ”§ Environment Variables

Create a `.env` file in the `backend/` directory:

```env
# Groq AI
GROQ_API_KEY=your_groq_api_key_here

# Server
PORT=3001

# MongoDB Atlas
MONGO_URI=mongodb+srv://<user>:<password>@cluster.mongodb.net/?appName=Cluster0

# Cloudinary
CLOUDINARY_CLOUD_NAME=your_cloud_name
CLOUDINARY_API_KEY=your_api_key
CLOUDINARY_API_SECRET=your_api_secret
```

---

## ğŸ› ï¸ Installation & Setup

### Prerequisites
- **Node.js** â‰¥ 18
- **npm** â‰¥ 9
- **ffmpeg** (required for voice transcription)
  ```bash
  # macOS
  brew install ffmpeg

  # Ubuntu / Debian
  sudo apt install ffmpeg
  ```

### 1. Clone the Repository
```bash
git clone https://github.com/your-org/AskMyNotes.git
cd AskMyNotes
```

### 2. Install Backend Dependencies
```bash
cd backend
npm install
```

### 3. Install Frontend Dependencies
```bash
cd ../frontend
npm install
```

### 4. Configure Environment
Copy and fill in your credentials:
```bash
cp backend/.env.example backend/.env
# Edit backend/.env with your keys
```

---

## â–¶ï¸ Running the App

Open **two terminal windows**:

**Terminal 1 â€” Backend**
```bash
cd backend
npm run dev
# âœ…  Server running at http://localhost:3001
# âœ…  MongoDB connected
# âœ…  Whisper model loaded and ready!
```

**Terminal 2 â€” Frontend**
```bash
cd frontend
npm run dev
# âœ  Local:   http://localhost:5173/
```

Then open **http://localhost:5173** in your browser.

> On first backend start, Whisper downloads the `whisper-tiny.en` ONNX model (~150 MB). This is cached in `backend/.cache/` for subsequent runs.

---

## ğŸ“¡ API Reference

All routes are prefixed with `/api`.

### Health Check
| Method | Endpoint | Description |
|---|---|---|
| `GET` | `/api/health` | Returns server uptime, subject count, and file count |

### Upload
| Method | Endpoint | Body | Description |
|---|---|---|---|
| `POST` | `/api/upload` | `multipart/form-data` â€” `files[]`, `subjectId`, `subjectName` | Upload up to 20 PDF/TXT files. Extracts text, uploads to Cloudinary, saves to MongoDB |
| `POST` | `/api/clear-subject` | `{ subjectId }` | Deletes all files for a subject from MongoDB and Cloudinary |

### Chat
| Method | Endpoint | Body | Description |
|---|---|---|---|
| `POST` | `/api/chat` | `{ subjectId, question, subjectName? }` | Returns AI answer strictly grounded in the subject's notes |

**Response format:**
```json
{
  "notFound": false,
  "answer": "Markdown answer string",
  "confidence": "High | Medium | Low",
  "evidence": ["Direct quote from notes..."],
  "citations": ["filename.pdf"]
}
```

### Study Mode
| Method | Endpoint | Body | Description |
|---|---|---|---|
| `POST` | `/api/study-mode` | `{ subjectId, subjectName?, fileName? }` | Generates a full study dashboard |

**Response format:**
```json
{
  "notes": "# Markdown Summary\n...",
  "mcqs": [
    {
      "question": "...",
      "options": ["A", "B", "C", "D"],
      "correctKey": "A",
      "explanation": "...",
      "citation": "filename.pdf"
    }
  ],
  "shortAnswer": [
    {
      "question": "...",
      "answer": "...",
      "citation": "filename.pdf"
    }
  ]
}
```

### Transcription
| Method | Endpoint | Body | Description |
|---|---|---|---|
| `POST` | `/api/transcribe` | `multipart/form-data` â€” `audio` (webm/wav/mp3) | Converts audio to 16 kHz WAV via ffmpeg, runs Whisper ASR, returns transcript |

**Response format:**
```json
{ "text": "Transcribed speech text here" }
```

---

## ğŸ—ƒï¸ Database Schema

### `Subject`
```
subjectId  String  (unique, indexed)
name       String
icon       String  (default: "ğŸ“˜")
color      String  (default: "s0")
createdAt  Date
```

### `File`
```
subjectId           String  (indexed)
fileName            String
cloudinaryUrl       String
cloudinaryPublicId  String
extractedText       String
uploadedAt          Date
```

### `ChatHistory`
```
subjectId   String  (indexed)
question    String
response    Mixed   (full Groq JSON response)
createdAt   Date
```

---

## ğŸ–¥ï¸ Frontend Pages

| Page | Route (internal) | Description |
|---|---|---|
| **Sign In** | auth gate | Email + password login; session stored in `localStorage` |
| **Sign Up** | auth gate | Registration; user list stored in `localStorage` |
| **Setup** | `setup` | Create/edit subjects (name, icon, color), upload files, trigger backend upload |
| **Study** | `study` | View AI-generated Markdown summary, take MCQ quiz, review flashcards, export PDF |
| **Chat** | `chat` | Ask questions via text or ğŸ¤ voice; answers rendered in Markdown with evidence citations |

---

## ğŸ” Authentication

Authentication is handled entirely client-side using **localStorage**:
- User accounts are stored under the key `askmynotes_users`
- Active session is stored under `askmynotes_session`
- Passwords are stored as plain text in localStorage (suitable for hackathon / demo use)
- On refresh, the session is restored automatically

> âš ï¸ **For production use**, replace this with a proper backend auth solution (JWT, OAuth, etc.)

---

## ğŸ§  How the AI Works

1. **Extraction** â€” When a file is uploaded, all text is extracted server-side (`pdf-parse` for PDFs, `fs.readFileSync` for TXT files) and stored in MongoDB.
2. **Context Assembly** â€” On every chat or study request, `getSubjectText.js` aggregates the `extractedText` of all files for the requested `subjectId` from MongoDB.
3. **Prompt Engineering** â€” The full text is injected into a structured prompt that instructs the Llama 3.3 70B model to answer **only** from the provided notes and respond in a strict JSON schema.
4. **Response Format** â€” All AI responses are requested in `json_object` mode via the Groq API and parsed server-side before being returned to the frontend.

---

## ğŸ™ï¸ Voice Transcription Flow

```
Browser (getUserMedia) 
  â†’ MediaRecorder (WebM blob)
    â†’ POST /api/transcribe
      â†’ ffmpeg converts â†’ 16 kHz mono WAV
        â†’ WAV parsed to Float32Array
          â†’ Whisper ONNX pipeline
            â†’ { text: "..." } returned to frontend
              â†’ auto-populates chat input
```

The Whisper model (`onnx-community/whisper-tiny.en`) is downloaded once (~150 MB) into `backend/.cache/` and reused for all subsequent runs.

---

## ğŸš§ Known Limitations

- Auth uses localStorage â€” **not suitable for production** without a real backend auth layer.
- Maximum **3 subjects** per session (by design).
- Whisper `tiny.en` is fast but less accurate than larger models; English-only.
- Cloudinary upload failure is **non-fatal** â€” the app continues if Cloudinary is unavailable.
- No real-time streaming of AI responses (full response is returned at once).

---

## ğŸ¤ Contributing

1. Fork the repo and create a feature branch.
2. Run both `backend` and `frontend` dev servers locally.
3. Make your changes and test thoroughly.
4. Open a pull request with a clear description of what you changed and why.

---
